{"cells":[{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["import numpy as np\n","from torch import hub\n","from torch import from_numpy\n","import torch.onnx\n","import pyaudio\n","\n","# PyAudio Setup\n","num_samples = 3024\n","FORMAT = pyaudio.paInt16\n","CHANNELS = 1\n","SAMPLE_RATE = 16000\n","CHUNK = int(SAMPLE_RATE / 10)\n","\n","audio = pyaudio.PyAudio()\n","recording_length = 15 # in seconds\n","time_between_recordings = 300\n","confidence_threshold = 0.92\n","conf_length = 20 # how much measurements to take\n","conf_enough = 10\n","\n","window = None\n","lock_vad = None\n","\n","cur_feeling = None\n","\n","\n","def int2float(sound):\n","    abs_max = np.abs(sound).max()\n","    sound = sound.astype('float32')\n","    if abs_max > 0:\n","        sound *= 1/abs_max\n","    sound = sound.squeeze()  # depends on the use case\n","    return sound\n","\n","def export_silero_vad():\n","    # model setup\n","    model, utils = hub.load(repo_or_dir='snakers4/silero-vad',\n","                              model='silero_vad',\n","                              force_reload=True)\n","\n","    (get_speech_timestamps,\n","    save_audio,\n","    read_audio,\n","    VADIterator,\n","    collect_chunks) = utils\n","    stream = audio.open(format=FORMAT,\n","                    channels=CHANNELS,\n","                    rate=SAMPLE_RATE, \n","                    input=True,\n","                    frames_per_buffer=CHUNK,\n","                    input_device_index=1\n","                    )\n","\n","    info = audio.get_host_api_info_by_index(0)\n","    numdevices = info.get('deviceCount') \n","    for i in range(0, numdevices):\n","        if (audio.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n","            print(\"Input Device id \", i, \" - \", audio.get_device_info_by_host_api_device_index(0, i).get('name'))\n","\n","    audio_chunk = stream.read(num_samples, exception_on_overflow = False)    \n","    # takes significant amount of time\n","    # if done on the fly in the same thread, brings noticeable artifacts to \n","    audio_int16 = np.frombuffer(audio_chunk, np.int16)\n","    audio_float32 = int2float(audio_int16)\n","    # get the confidences\n","    torch_input = (from_numpy(audio_float32), 16000)\n","    new_confidence = model(from_numpy(audio_float32), 16000).item()\n","    \n","    # torch.onnx.export(model,               # model being run\n","    #              (from_numpy(audio_float32), 16000),  # model input (or a tuple for multiple inputs)\n","    #               \"silero_vad.onnx\",   # where to save the model (can be a file or file-like object)\n","    #               export_params=True,        # store the trained parameter weights inside the model file\n","    #               opset_version=10,          # the ONNX version to export the model to\n","    #               do_constant_folding=True,  # whether to execute constant folding for optimization\n","    #               input_names = ['input', 'freq'],   # the model's input names\n","    #               output_names = ['output'], # the model's output names\n","    #               dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n","    #                             'output' : {0 : 'batch_size'}})\n","    # export_output = torch.onnx.dynamo_export(model, torch_input)\n","    # export_output.save(\"my_image_classifier.onnx\")\n","\n","    print(new_confidence)\n","    print('silero_vad exported')   \n","    stream.close()\n","\n","    return audio_float32\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\ioci/.cache\\torch\\hub\\master.zip\n"]},{"name":"stdout","output_type":"stream","text":["Input Device id  0  -  Microsoft Sound Mapper - Input\n","Input Device id  1  -  Микрофон (2- Samson GoMic)\n","Input Device id  2  -  Микрофон (NVIDIA Broadcast)\n","0.21194827556610107\n","silero_vad exported\n"]}],"source":["audio_float32 = export_silero_vad()"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["import onnx\n","\n","onnx_model = onnx.load(\"silero_vad.onnx\")\n","onnx.checker.check_model(onnx_model, full_check=True)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input\n","sr\n","h\n","c\n"]}],"source":["import onnxruntime\n","\n","ort_session = onnxruntime.InferenceSession(\"silero_vad.onnx\", providers=[\"CPUExecutionProvider\"])\n","\n","def to_numpy(tensor):\n","    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n","\n","inputs = ort_session.get_inputs()\n","for i in inputs:\n","    print(i.name)"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 0.5378049   0.827439    0.5        ... -0.11280487  0.0554878\n","  0.31036586]\n"]}],"source":["print(audio_float32)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["from typing import Callable, List\n","import warnings\n","\n","languages = ['ru', 'en', 'de', 'es']\n","\n","\n","class OnnxWrapper():\n","\n","    def __init__(self, path, force_onnx_cpu=False):\n","        import numpy as np\n","        import onnxruntime\n","\n","        opts = onnxruntime.SessionOptions()\n","        opts.inter_op_num_threads = 1\n","        opts.intra_op_num_threads = 1\n","\n","        if force_onnx_cpu and 'CPUExecutionProvider' in onnxruntime.get_available_providers():\n","            self.session = onnxruntime.InferenceSession(path, providers=['CPUExecutionProvider'], sess_options=opts)\n","        else:\n","            self.session = onnxruntime.InferenceSession(path, sess_options=opts)\n","\n","        self.reset_states()\n","        self.sample_rates = [8000, 16000]\n","\n","    def _validate_input(self, x, sr: int):\n","        if x.dim() == 1:\n","            x = x.unsqueeze(0)\n","        if x.dim() > 2:\n","            raise ValueError(f\"Too many dimensions for input audio chunk {x.dim()}\")\n","\n","        if sr != 16000 and (sr % 16000 == 0):\n","            step = sr // 16000\n","            x = x[:,::step]\n","            sr = 16000\n","\n","        if sr not in self.sample_rates:\n","            raise ValueError(f\"Supported sampling rates: {self.sample_rates} (or multiply of 16000)\")\n","\n","        if sr / x.shape[1] > 31.25:\n","            raise ValueError(\"Input audio chunk is too short\")\n","\n","        return x, sr\n","\n","    def reset_states(self, batch_size=1):\n","        self._h = np.zeros((2, batch_size, 64)).astype('float32')\n","        self._c = np.zeros((2, batch_size, 64)).astype('float32')\n","        self._last_sr = 0\n","        self._last_batch_size = 0\n","\n","    def __call__(self, x, sr: int):\n","\n","        x, sr = self._validate_input(x, sr)\n","        batch_size = x.shape[0]\n","\n","        if not self._last_batch_size:\n","            self.reset_states(batch_size)\n","        if (self._last_sr) and (self._last_sr != sr):\n","            self.reset_states(batch_size)\n","        if (self._last_batch_size) and (self._last_batch_size != batch_size):\n","            self.reset_states(batch_size)\n","\n","        if sr in [8000, 16000]:\n","            ort_inputs = {'input': x.numpy(), 'h': self._h, 'c': self._c, 'sr': np.array(sr, dtype='int64')}\n","            ort_outs = self.session.run(None, ort_inputs)\n","            out, self._h, self._c = ort_outs\n","        else:\n","            raise ValueError()\n","\n","        self._last_sr = sr\n","        self._last_batch_size = batch_size\n","\n","        out = torch.tensor(out)\n","        return out\n","\n","    def audio_forward(self, x, sr: int, num_samples: int = 512):\n","        outs = []\n","        x, sr = self._validate_input(x, sr)\n","\n","        if x.shape[1] % num_samples:\n","            pad_num = num_samples - (x.shape[1] % num_samples)\n","            x = torch.nn.functional.pad(x, (0, pad_num), 'constant', value=0.0)\n","\n","        self.reset_states(x.shape[0])\n","        for i in range(0, x.shape[1], num_samples):\n","            wavs_batch = x[:, i:i+num_samples]\n","            out_chunk = self.__call__(wavs_batch, sr)\n","            outs.append(out_chunk)\n","\n","        stacked = torch.cat(outs, dim=1)\n","        return stacked.cpu()"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["model = OnnxWrapper('silero_vad.onnx', True)"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["sr = 16000\n","x = audio_float32"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# compute ONNX Runtime output prediction\n","\n","# x, sr = _validate_input(from_numpy(audio_float32), sr)\n","\n","_h = np.zeros((2, 1, 64)).astype('float32')\n","_c = np.zeros((2, 1, 64)).astype('float32')\n","_sr = np.array(sr, dtype='int64')\n","_x = np.expand_dims(x, axis = 0)\n","\n","ort_inputs = {'input': _x, 'sr': _sr, 'h': _h, 'c': _c}\n","ort_outs = ort_session.run(None, ort_inputs)\n","\n","# compare ONNX Runtime and PyTorch results\n","# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n","\n","# print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"data":{"text/plain":["0.21194823"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["ort_outs[0][0][0]"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\ioci\\Desktop\n","C:\\Users\\ioci\\Desktop\\records\n"]}],"source":["import os\n","\n","# the above is valid on Windows (after 7) but if you want it in os normalized form:\n","desktop = os.path.normpath(os.path.expanduser(\"~/Desktop\"))\n","print(desktop)\n","p = os.path.normpath(os.path.join(desktop, 'records'))\n","print(p)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["record_name = 'name'\n","result = 'result'\n","records_path = 'records'\n","records_txt = '_records.txt'"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["C:\\Users\\ioci\\Desktop\\records\n"]}],"source":["p = os.path.normpath(os.path.join(desktop, records_path))\n","print(p)\n","try:\n","    with open(os.path.join(p, records_txt), 'a') as f:\n","        f.write(record_name + ': ' + result + '\\n')\n","except FileNotFoundError:\n","    p = os.path.normpath(os.path.join(desktop, 'records'))\n","    os.mkdir(p)\n","    with open(os.path.join(p, records_txt), 'w') as f:\n","        f.write(record_name + ': ' + result + '\\n')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","2\n"]}],"source":["a = {}\n","a[1] = 'a'\n","a[2] = 'b'\n","\n","for i in a:\n","    print(i)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(a.keys()))"]}],"metadata":{"kernelspec":{"display_name":"vad_app","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
